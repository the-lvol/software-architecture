{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typescript/Javascript dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    ['git', 'clone', 'https://github.com/supabase/supabase.git'], \n",
    "    cwd='../../../', \n",
    "    stdout=subprocess.DEVNULL, \n",
    "    stderr=subprocess.DEVNULL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/skott-supabase.json'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "subprocess.run(\n",
    "    ['skott', '--displayMode=json'], \n",
    "    cwd='../../../supabase', \n",
    "    stdout=subprocess.DEVNULL, \n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "shutil.copy('../../../supabase/skott.json', './data/skott-supabase.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import copy\n",
    "\n",
    "# data_path = './data/skott.json'\n",
    "# # data_path = './data/skott-apps.json'\n",
    "\n",
    "# with open(data_path, 'r') as f:\n",
    "#     json_data = json.load(f)\n",
    "\n",
    "# temp_json_data = copy.deepcopy(json_data)\n",
    "# for key in temp_json_data:\n",
    "#     if key.startswith('examples/') or key.startswith('docker') or key.startswith('tests') or key.startswith('playwright-tests') or 'config.' in key:\n",
    "#         del json_data[key]\n",
    "\n",
    "# data = {\n",
    "#     'nodes': [],\n",
    "#     'links': [],\n",
    "# }\n",
    "\n",
    "# for key in json_data:\n",
    "#     data['nodes'].append({\n",
    "#         'id': key\n",
    "#     })\n",
    "\n",
    "#     for adjacent in json_data[key]['adjacentTo']:\n",
    "#         data['links'].append({\n",
    "#             'source': key,\n",
    "#             'target': adjacent,\n",
    "#             'waeight': 1,\n",
    "#         })\n",
    "\n",
    "# print(f\"nodes: {len(data['nodes'])}\")\n",
    "# print(f\"links: {len(data['links'])}\")\n",
    "\n",
    "\n",
    "# # output_file = \"./data.json\"\n",
    "\n",
    "# # with open(output_file, 'w') as f:\n",
    "# #     json.dump(data, f)\n",
    "\n",
    "# # print(f\"Data saved as JSON file: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_directory_paths(top_location, directory_name, excludes=[]):\n",
    "    directory_paths = set()\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(top_location):\n",
    "        for dirname in dirnames:\n",
    "            if any(exclude in dirpath for exclude in excludes):\n",
    "                continue\n",
    "\n",
    "            if directory_name in dirname:\n",
    "                directory_paths.add(dirpath)\n",
    "\n",
    "    return directory_paths    \n",
    "\n",
    "\n",
    "def find_files_in_folders(top_location, filename, excludes=[]):\n",
    "    file_paths = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(top_location):\n",
    "        if any(exclude in dirpath for exclude in excludes):\n",
    "            continue\n",
    "\n",
    "        if any(filename in f for f in filenames):\n",
    "            file_paths.append(dirpath)\n",
    "\n",
    "    return file_paths    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folders = find_directory_paths('/workspaces/supabase', 'test', excludes=['node_modules', 'images', 'examples'])\n",
    "print(test_folders)\n",
    "print(len(test_folders))\n",
    "\n",
    "jest_test_files = find_files_in_folders('/workspaces/supabase', 'jest', excludes=['node_modules'])\n",
    "print(jest_test_files)\n",
    "print(len(jest_test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def run_and_copy_jest_coverage_summary(jest_paths, store_path):\n",
    "    for jest_path in jest_paths:\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                ['jest', '--coverage', '--coverageReporters=json-summary'], \n",
    "                cwd=jest_path, \n",
    "                stdout=subprocess.DEVNULL, \n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(store_path):\n",
    "                os.makedirs(store_path)\n",
    "\n",
    "            shutil.copy(f'{jest_path}/coverage/coverage-summary.json', f'{store_path}{\"-\".join(jest_path.split(\"/\")[2:])}-coverage-summary.json')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "run_and_copy_jest_coverage_summary(jest_test_files, './data/code-test-coverage/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydriller import Repository\n",
    "\n",
    "def get_all_commits(repository_file_location):\n",
    "    return list(Repository(repository_file_location).traverse_commits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the example commits below\n",
    "\n",
    "```\n",
    "commit (new file)\n",
    "    old_path = null\n",
    "    new_path = path\n",
    "\n",
    "commit (path changed)\n",
    "    old_path = path\n",
    "    new_path = another_path\n",
    "\n",
    "commit (standard)\n",
    "    old_path = path\n",
    "    new_path = path\n",
    "\n",
    "commit (old existing path)\n",
    "    old_path = None\n",
    "    new_path = alredy_seen_path\n",
    "```\n",
    "\n",
    "Because of the last case, we have to process the commits in order, and delete an entry if the path changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def file_change_count_list(commits, contains_file_endings=[]):\n",
    "    file_change_count = {}\n",
    "\n",
    "    change_count_key = 'change-count'\n",
    "    last_modified_key = 'last-modified'\n",
    "\n",
    "    for commit in commits:\n",
    "        for modification in commit.modified_files:\n",
    "            if not modification.filename.endswith(tuple(contains_file_endings)):\n",
    "                continue\n",
    "\n",
    "            if modification.old_path in file_change_count and not modification.new_path in file_change_count:\n",
    "                data_to_move = file_change_count[modification.old_path]\n",
    "                del file_change_count[modification.old_path]\n",
    "\n",
    "                file_change_count[modification.new_path] = {\n",
    "                    change_count_key: data_to_move[change_count_key] + 1,\n",
    "                    last_modified_key: commit.committer_date.strftime('%m/%d/%Y, %H:%M:%S'),\n",
    "                }\n",
    "            elif not modification.new_path in file_change_count:\n",
    "                file_change_count[modification.new_path] = {\n",
    "                    change_count_key: 1,\n",
    "                    last_modified_key: commit.committer_date.strftime('%m/%d/%Y, %H:%M:%S')\n",
    "                }\n",
    "            else:\n",
    "                file_change_count[modification.new_path][change_count_key] += 1\n",
    "                file_change_count[modification.new_path][last_modified_key] = commit.committer_date.strftime('%m/%d/%Y, %H:%M:%S')\n",
    "\n",
    "    return file_change_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27836\n"
     ]
    }
   ],
   "source": [
    "all_commits = get_all_commits('../../../supabase')\n",
    "print(len(all_commits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5658\n"
     ]
    }
   ],
   "source": [
    "# Takes ~13 minuets\n",
    "\n",
    "# file_changes = file_change_count_list(all_commits[:250], contains_file_endings=['ts', 'tsx', 'js', 'jsx'])\n",
    "file_changes = file_change_count_list(all_commits, contains_file_endings=['ts', 'tsx', 'js', 'jsx'])\n",
    "print(len(file_changes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# sorted_file_changes = {k: v for k, v in sorted(file_changes.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "with open('./data/code-churn-v3.json', 'w') as file:\n",
    "    json.dump(file_changes, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
